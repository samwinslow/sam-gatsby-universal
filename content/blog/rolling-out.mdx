---
slug: "blog/rolling-out"
date: "2020-09-09"
title: "Rolling Out the Welcome Mat for AI"
category: "blog"
copy: "A set of guiding principles I work to uphold."
image: 000094390018-ig.jpg
---

Rolling Out the Welcome Mat for AI

Should we think of ourselves as collaborators, caretakers, or competitors with regards to AI? The fact that virtual assistants, recommendation algorithms, and facial recognition systems are already central to the devices we use every day makes this question even more pressing.

I came across John Danaher's article on competitive cognitive artifacts, which considers whether artificially intelligent tools pose an existential threat to humanity. Let's first ask what we mean by artifacts, and look to Don Norman for an answer: humans have the unique ability to "modify the environment in which they live through the creation of artifacts" (Norman 1991, quoting Cole 1990). Artifacts are tools, or objects that give us power beyond our physical capacities. Danaher's article uses the simple example of a shovel. You can move a whole lot more dirt with one than you could with your bare hands.

Cognitive artifacts are artifacts that "maintain, display or operate upon information in order to serve a representational function." Let's call them artifacts which operate on systems of record. A subset of artifacts, cognitive artifacts integrate with our capacities to think and communicate. The process of representing information can be a declaration of knowledge, a declaration of ability, and even a declaration of superiority. In other words, the use of a cognitive artifact is to arrive at "I know," with all its many definitions.

The next distinction is between complementary and competitive cognitive artifacts, and a Danaher quote best explains it:

> Complementary Cognitive Artifacts: These are artifacts that complement human intelligence in such a way that their use amplifies and improves our ability to perform cognitive tasks and once the user has mastered the physical artifact they can use a virtual/mental equivalent to perform the same cognitive task at a similar level of skill, e.g. an abacus.

> Competitive Cognitive Artifacts: These are artifacts that amplify and improve our abilities to perform cognitive tasks when we have use of the artifact but when we take away the artifact we are no better (and possibly worse) at performing the cognitive task than we were before.

Danaher seems to side with the belief that we are, collectively, producing more competitive and fewer complementary cognitive artifacts as we advance as a society, and that this is inherently bad. Into the "competitive" bucket he lumps systems like Google Maps (which replaces directional awareness), Netflix's recommendation system (which replaces free choice), and anything by which "things are done for us not by us."

By putting business decisions in the hands of AI, we're even leaving innovation itself up to machines.

But is there an implied contradiction: more computing power, year after year (see Moore's Law), means an expanding problem space, not a shrinking one. For every biased algorithm that radicalizes YouTube viewers (thereby competing with the viewers' natural ability to judge right and wrong), there's an exponentially shrinking barrier to entry for producing a different algorithm that challenges it.

Ideally, it would be possible to create an algorithm which delivers content at scale in a way that's compatible with democracy, equitable for creators, furthers the UN Human Development Goals, spreads awareness about autism, or any other arbitrary benchmark you hold it to. This is the beauty of an expanded solution space.

The above conclusion results if, all else equal, we consider computing power to be the only input to production. This is horribly naïve, of course. The tools we use to create new and useful artifacts are ever more complex, and our ability to design better tools is constrained by the artifacts we have at our disposal. And wow, are those artifacts distributed unequally! Raw materials may be less important in creating cognitive artifacts as opposed to physical ones — but social capital (e.g. education, cultural norms, and personal connections) defines what we imagine to be possible within any frame.

And so, as long as inequality exists among people, we will build technologies which disproportionally affect certain groups. We will correct one inequality while raising another, like a game of Whack-a-Mole. But just because this is inevitable does not mean we shouldn't play the game.

The constant factor is not that humans are unequal or that technology is an equalizer; it is that all of our activity is additive (see the second law of thermodynamics). A law which was repealed was at one point legal; a fight which was settled still happened. Because power is maintained by the preservation or destruction of systems of record (from balancing an accounting ledger, to the Nazis burning books) our reliance on these systems will only continue to increase. This has been true since the dawn of the post-industrial age.

Immediately evident threats sometimes emerge in the form of competitive cognitive artifacts (e.g. deepfakes, in which a politician's face and voice might be deeply imitated in order to spread misinformation - ArXiv). I'm extending the definition of cognitive artifacts to include both the form and contents of systems of record, because artifacts can create artifacts which create artifacts and so on.

When such threatening artifacts come about, we do not have the luxury to imagine that "when we take away the artifact we are no better (and possibly worse) at performing the cognitive task than we were before" (Danaher). Being able to 'take away the artifact' assumes we have the time, knowledge, or motivation to do so; and in a complex system, there is no such concept as 'just take it away' without 2nd- or nth-order effects. What we can do instead is create more cognitive artifacts to neutralize the threat, e.g. deepfake detection algorithms.

To return to my opening question — should we become collaborators, caretakers, or competitors with regards to AI — the answer is a combination of all three.

1. As optimistic yet skeptical collaborators, we should continue to study novel applications of all new technologies, including AI.
2. As caretakers, we should assure that both the risks and benefits of AI are accrued to as large and diverse a section of humanity as possible.
3. As competitors, we should think like Michael Jordan: "Talent wins games, but teamwork and intelligence win championships." We've already lost if we focus on talent alone. As people of intelligence, we need to lead by example and explicitly focus on how to organize society around the challenges ahead.